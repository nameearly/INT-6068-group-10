# 这段代码**不是强化学习算法**，而是用来直观预览强化学习后结果的程序，**给强化学习算法提供一个可交互的 “虚拟世界”**，我们先把「环境参数」调好、试顺 → 再跑强化学习算法训练 → 最后用这套参数看最终效果。

> 一个基于 `Gymnasium`（新版 OpenAI Gym）写的
> 
> **多智能体三维无人机配送 强化学习仿真环境**


目前静态仿真（结合体.py）已成功运行，效果完整展示了多智能体无人机导航的核心逻辑。
---
  第一步：打开终端

  按 Win + R，输入 cmd，回车。

  ---
  第二步：进入仿真脚本所在目录

  在终端中粘贴以下命令（注意路径含中文和空格，必须加引号）：

  cd /d "【路径】\简化仿真模拟环境下的结果\静态模拟"

  ---
  第三步：运行仿真

  python 结合体.py

  ---
  第四步：观察效果

  稍等 2-3 秒后，一个 3D matplotlib 窗口会弹出，你会看到：

  - 灰色高楼群构成的城市场景（1000×1000m）
  - 3 架无人机（蓝/绿/紫）实时移动并留下飞行轨迹
  - 金色五角星标记每架无人机的目标位置
  - 左上角显示实时状态：步数、电量、碰撞次数、与目标距离

  程序会自动运行 5 个 episode，全部结束后自动退出。中途想停止直接关闭窗口即可。

  ---
  如果提示 python 找不到，把第三步改为 D:\Python\python.exe 结合体.py




  # 二、代码整体分成哪几个模块？

我按**功能 + 代码结构**，拆成 6 个清晰模块：

## 1. 依赖导入 & 全局设置

python

```
import numpy as np
from gymnasium import spaces, Env
...
```

- 导入数值计算、强化学习环境基类、3D 画图、空间搜索等库
- 设置中文字体、关闭警告

## 2. 环境主体类：多无人机配送环境

python

```
class MultiAgentDroneDeliveryEnv(Env):
```

- 继承 `gym.Env`，是**标准强化学习环境**
- 管理：多架无人机、城市、障碍物、起点终点、电池、碰撞

## 3. 初始化模块 `__init__`（最核心配置）

做 4 件事：

1. **定义动作空间**
    
    - 每架无人机：3 维连续动作（x/y/z 方向控制）
    
2. **定义观察空间**
    
    - 每架无人机能 “看到”：自己位置、速度、目标、障碍物、其他无人机、电池
    
3. **生成世界**
    
    - 生成城市建筑（障碍物）、随机起点、随机目标点
    
4. **设置规则**
    
    - 最大速度、电池消耗、安全距离、目标半径、最大步数
- city_size = 1000 城市大小
- max_speed = 10.0 无人机最大速度
- safety_distance = 20.0 避障安全距离
- goal_radius = 5.0 目标半径
- battery_capacity = 100 电池容量
  > 把 city_size = 500 变小，画面更紧凑
  > goal_radius = 10 变大，无人机更容易到达目标
  > max_speed = 5 变慢，飞得更稳

## 4. 环境初始化 / 重置模块 `reset()`

- 每局开始时调用
- 把无人机放回起点、电池充满、清空轨迹
- 返回**初始观测**给强化学习算法

## 5. 核心交互模块 `step()` （RL 最关键函数）

算法输入：`actions` 每架无人机的控制指令

环境输出：`观测、奖励、是否结束、信息`

内部流程：

1. 限制动作、更新无人机速度 & 位置
2. 扣电池、记录飞行轨迹
3. 检测障碍物、无人机间距离
4. **计算奖励**
5. 判断：是否到达目标、是否撞楼、是否超时
6. 返回下一步状态

## 6. 工具函数模块（内部辅助）

- `_generate_city_buildings()`：生成三维城市楼房当障碍物
- `_select_start_positions_and_goals()`：随机分配起点 & 终点
- `_get_obs()`：给每架无人机构造 “看到的画面”
- `_calculate_reward()`：**设计奖励函数**（RL 灵魂）
- `_update_closest_obstacle_distances()`：实时算最近障碍物

## 7. 可视化模块（代码里有预留）

- 3D 画城市、建筑、无人机、飞行轨迹

**本身不含 DQN、PPO 等算法，只负责：状态、动作、奖励、仿真。**
